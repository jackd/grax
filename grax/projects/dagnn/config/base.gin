import huf.configurables

import grax.config
import grax.graph_utils.transforms
import grax.hk_utils
import grax.projects.dagnn.modules

include "grax_config/single/data/transformed.gin"

node_features_transform = [
    @grax.graph_utils.transforms.row_normalize,
    @grax.graph_utils.transforms.to_format
]
graph_transform = [
    @grax.graph_utils.transforms.add_identity,
    @grax.graph_utils.transforms.symmetric_normalize,
    @grax.graph_utils.transforms.scale,
]

grax.graph_utils.transforms.to_format.fmt = %fmt
grax.graph_utils.transforms.scale.scale = %adj_factor

fmt = "dense"

module_fun = @dagnn.DAGNN

dagnn.DAGNN.node_transform = @grax.hk_utils.mlp
dagnn.DAGNN.num_propagations = %num_propagations
dagnn.DAGNN.gate_activation = %gate_activation
dagnn.DAGNN.adaptive = %adaptive
dagnn.DAGNN.scale_factor = %logit_factor
dagnn.DAGNN.scale_method = %scale_method
grax.hk_utils.mlp.num_classes = %num_classes
grax.hk_utils.mlp.hidden_filters = %filters
grax.hk_utils.mlp.dropout_rate = %dropout_rate
grax.hk_utils.mlp.input_dropout_rate = %input_dropout_rate
grax.hk_utils.mlp.use_batch_norm = %use_batch_norm

input_dropout_rate = None
use_batch_norm = False

gate_activation = @jax.nn.sigmoid

optimizer = @optax.chain_star()
optax.chain_star.transforms = [
    @optax.additive_weight_decay(),
    @optax.adam(),
]

optax.adam.learning_rate = %lr
optax.additive_weight_decay.weight_decay = %weight_decay

logit_factor = 1.0
adj_factor = 1.0

adaptive = True
lr = 1e-2
filters = 64

steps = 1000
patience = 100
scale_method = 'all'

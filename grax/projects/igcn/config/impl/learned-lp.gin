import huf.configurables

import grax.hk_utils
import grax.optax_utils
import grax.projects.igcn.data
import grax.projects.igcn.modules

include "grax_config/single/data/transformed.gin"

module_fun = @igcn.LearnedIGCN
igcn.LearnedIGCN.tol = (%tol, %test_tol)
igcn.LearnedIGCN.maxiter = (%maxiter, %test_maxiter)
igcn.IGCN.num_classes = %num_classes
igcn.IGCN.smooth_only = %smooth_only
igcn.IGCN.head_transform = %head_transform
igcn.IGCN.tail_transform = %tail_transform
head_transform = @grax.hk_utils.mlp
tail_transform = None

smooth_only = True


optimizer = @optax.chain_star()
optax.chain_star.transforms = [
    @grax.optax_utils.partition(),  # weight decay is not epsilon
    @optax.adam(),
]

grax.optax_utils.partition.predicate = @igcn.if_e0
grax.optax_utils.partition.if_false = @optax.additive_weight_decay()

node_features_transform = [
    @grax.graph_utils.transforms.row_normalize,
    @grax.graph_utils.transforms.to_format
]
grax.graph_utils.transforms.to_format.fmt = %fmt

graph_transform = []
split_data/gin.singleton.constructor = @igcn.data.get_learned_split_data

igcn.data.get_learned_split_data.data = @grax.problems.single.transformed_simple()

grax.problems.single.transformed_simple.as_split = False
optax.adam.learning_rate = %lr
optax.additive_weight_decay.weight_decay = %weight_decay

grax.hk_utils.mlp.dropout_rate = %dropout_rate
grax.hk_utils.mlp.use_batch_norm = %use_batch_norm
grax.hk_utils.mlp.use_layer_norm = %use_layer_norm
grax.hk_utils.mlp.use_gathered_batch_norm = %use_gathered_batch_norm
grax.hk_utils.mlp.renorm_scale = %renorm_scale
grax.hk_utils.mlp.input_dropout_rate = %input_dropout_rate
grax.hk_utils.mlp.hidden_filters = %filters

renorm_scale = True
lr = 1e-2
input_dropout_rate = None
filters = 64
use_batch_norm = False
use_layer_norm = False
use_gathered_batch_norm = False

tol = 1e-2
test_tol = %tol
maxiter = None
test_maxiter = 100000
normalize_features = True
fmt = "dense"
